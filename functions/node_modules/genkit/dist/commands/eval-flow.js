"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.evalFlow = void 0;
const eval_1 = require("@genkit-ai/tools-common/eval");
const utils_1 = require("@genkit-ai/tools-common/utils");
const commander_1 = require("commander");
const crypto_1 = require("crypto");
const promises_1 = require("fs/promises");
const EVAL_FLOW_SCHEMA = '{samples: Array<{input: any; reference?: any;}>}';
exports.evalFlow = new commander_1.Command('eval:flow')
    .description('evaluate a flow against configured evaluators using provided data as input')
    .argument('<flowName>', 'Name of the flow to run')
    .argument('[data]', 'JSON data to use to start the flow')
    .option('--input <filename>', 'JSON batch data to use to run the flow')
    .option('-a, --auth <JSON>', 'JSON object passed to authPolicy and stored in local state as auth', '')
    .option('-o, --output <filename>', 'Name of the output file to write evaluation results. Defaults to json output.')
    .option('--output-format <format>', 'The output file format (csv, json)', 'json')
    .option('-e, --evaluators <evaluators>', 'comma separated list of evaluators to use (by default uses all)')
    .option('-f, --force', 'Automatically accept all interactive prompts')
    .action(async (flowName, data, options) => {
    await (0, utils_1.runInRunnerThenStop)(async (runner) => {
        const evalStore = (0, eval_1.getEvalStore)();
        let exportFn = (0, eval_1.getExporterForString)(options.outputFormat);
        const allActions = await runner.listActions();
        const allEvaluatorActions = [];
        for (const key in allActions) {
            if ((0, utils_1.isEvaluator)(key)) {
                allEvaluatorActions.push(allActions[key]);
            }
        }
        const filteredEvaluatorActions = allEvaluatorActions.filter((action) => !options.evaluators ||
            options.evaluators.split(',').includes(action.name));
        if (filteredEvaluatorActions.length === 0) {
            if (allEvaluatorActions.length == 0) {
                utils_1.logger.error('No evaluators installed');
            }
            else {
                utils_1.logger.error(`No evaluators matched your specifed filter: ${options.evaluators}`);
                utils_1.logger.info(`All available evaluators: ${allEvaluatorActions.map((action) => action.name).join(',')}`);
            }
            return;
        }
        if (!data && !options.input) {
            utils_1.logger.error('No input data passed. Specify input data using [data] argument or --input <filename> option');
            return;
        }
        utils_1.logger.info(`Using evaluators: ${filteredEvaluatorActions.map((action) => action.name).join(',')}`);
        const confirmed = await (0, utils_1.confirmLlmUse)(filteredEvaluatorActions, options.force);
        if (!confirmed) {
            return;
        }
        const parsedData = await readInputs(data, options.input);
        const states = await runFlows(runner, flowName, parsedData);
        const runStates = states.map((s) => {
            return {
                state: s,
                hasErrored: !!s.operation.result?.error,
                error: s.operation.result?.error,
            };
        });
        if (runStates.some((s) => s.hasErrored)) {
            utils_1.logger.error('Some flows failed with errors');
        }
        const evalDataset = await fetchDataSet(runner, flowName, runStates, parsedData);
        const evalRunId = (0, crypto_1.randomUUID)();
        const scores = {};
        for (const action of filteredEvaluatorActions) {
            const name = (0, utils_1.evaluatorName)(action);
            utils_1.logger.info(`Running evaluator '${name}'...`);
            const response = await runner.runAction({
                key: name,
                input: {
                    dataset: evalDataset.filter((row) => !row.error),
                    evalRunId,
                    auth: options.auth ? JSON.parse(options.auth) : undefined,
                },
            });
            scores[name] = response.result;
        }
        const scoredResults = (0, eval_1.enrichResultsWithScoring)(scores, evalDataset);
        const metadata = (0, eval_1.extractMetricsMetadata)(filteredEvaluatorActions);
        const evalRun = {
            key: {
                actionId: flowName,
                evalRunId,
                createdAt: new Date().toISOString(),
            },
            results: scoredResults,
            metricsMetadata: metadata,
        };
        utils_1.logger.info(`Writing results to EvalStore...`);
        await evalStore.save(evalRun);
        if (options.output) {
            await exportFn(evalRun, options.output);
        }
    });
});
async function readInputs(data, filePath) {
    const parsedData = JSON.parse(data ? data : await (0, promises_1.readFile)(filePath, 'utf8'));
    if (Array.isArray(parsedData)) {
        return parsedData;
    }
    try {
        return eval_1.EvalFlowInputSchema.parse(parsedData);
    }
    catch (e) {
        throw new Error(`Error parsing the input. Please provide an array of inputs for the flow or a ${EVAL_FLOW_SCHEMA} object. Error: ${e}`);
    }
}
async function runFlows(runner, flowName, data) {
    const states = [];
    let inputs = Array.isArray(data)
        ? data
        : data.samples.map((c) => c.input);
    for (const d of inputs) {
        utils_1.logger.info(`Running '/flow/${flowName}' ...`);
        let state = (await runner.runAction({
            key: `/flow/${flowName}`,
            input: {
                start: {
                    input: d,
                },
            },
        })).result;
        if (!state?.operation.done) {
            utils_1.logger.info('Started flow run, waiting for it to complete...');
            state = await (0, utils_1.waitForFlowToComplete)(runner, flowName, state.flowId);
        }
        utils_1.logger.info('Flow operation:\n' + JSON.stringify(state.operation, undefined, '  '));
        states.push(state);
    }
    return states;
}
async function fetchDataSet(runner, flowName, states, parsedData) {
    let references = undefined;
    if (!Array.isArray(parsedData)) {
        const maybeReferences = parsedData.samples.map((c) => c.reference);
        if (maybeReferences.length === states.length) {
            references = maybeReferences;
        }
        else {
            utils_1.logger.warn('The input size does not match the flow states generated. Ignoring reference mapping...');
        }
    }
    const extractors = await (0, utils_1.getEvalExtractors)(flowName);
    return await Promise.all(states.map(async (s, i) => {
        const traceIds = s.state.executions.flatMap((e) => e.traceIds);
        if (traceIds.length > 1) {
            utils_1.logger.warn('The flow is split across multiple traces');
        }
        const traces = await Promise.all(traceIds.map(async (traceId) => runner.getTrace({
            env: 'dev',
            traceId,
        })));
        let inputs = [];
        let outputs = [];
        let contexts = [];
        traces.forEach((trace) => {
            inputs.push(extractors.input(trace));
        });
        if (s.hasErrored) {
            return {
                testCaseId: (0, crypto_1.randomUUID)(),
                input: inputs[0],
                error: s.error,
                reference: references?.at(i),
                traceIds,
            };
        }
        traces.forEach((trace) => {
            outputs.push(extractors.output(trace));
            contexts.push(extractors.context(trace));
        });
        return {
            testCaseId: (0, crypto_1.randomUUID)(),
            input: inputs[0],
            output: outputs[0],
            context: JSON.parse(contexts[0]),
            reference: references?.at(i),
            traceIds,
        };
    }));
}
//# sourceMappingURL=eval-flow.js.map