"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.evalRun = void 0;
const eval_1 = require("@genkit-ai/tools-common/eval");
const utils_1 = require("@genkit-ai/tools-common/utils");
const commander_1 = require("commander");
const crypto_1 = require("crypto");
const promises_1 = require("fs/promises");
exports.evalRun = new commander_1.Command('eval:run')
    .description('evaluate provided dataset against configured evaluators')
    .argument('<dataset>', 'Dataset to evaluate on (currently only supports JSON)')
    .option('--output <filename>', 'name of the output file to write evaluation results. Defaults to json output.')
    .option('--output-format <format>', 'The output file format (csv, json)', 'json')
    .option('--evaluators <evaluators>', 'comma separated list of evaluators to use (by default uses all)')
    .option('--force', 'Automatically accept all interactive prompts')
    .action(async (dataset, options) => {
    await (0, utils_1.runInRunnerThenStop)(async (runner) => {
        const evalStore = (0, eval_1.getEvalStore)();
        const exportFn = (0, eval_1.getExporterForString)(options.outputFormat);
        utils_1.logger.debug(`Loading data from '${dataset}'...`);
        const evalDataset = JSON.parse((await (0, promises_1.readFile)(dataset)).toString('utf-8')).map((testCase) => ({
            ...testCase,
            testCaseId: testCase.testCaseId || (0, crypto_1.randomUUID)(),
            traceIds: testCase.traceIds || [],
        }));
        const allActions = await runner.listActions();
        const allEvaluatorActions = [];
        for (const key in allActions) {
            if ((0, utils_1.isEvaluator)(key)) {
                allEvaluatorActions.push(allActions[key]);
            }
        }
        const filteredEvaluatorActions = allEvaluatorActions.filter((action) => !options.evaluators ||
            options.evaluators.split(',').includes(action.name));
        if (filteredEvaluatorActions.length === 0) {
            if (allEvaluatorActions.length == 0) {
                utils_1.logger.error('No evaluators installed');
            }
            else {
                utils_1.logger.error(`No evaluators matched your specifed filter: ${options.evaluators}`);
                utils_1.logger.info(`All available evaluators: ${allEvaluatorActions.map((action) => action.name).join(',')}`);
            }
            return;
        }
        utils_1.logger.info(`Using evaluators: ${filteredEvaluatorActions.map((action) => action.name).join(',')}`);
        const confirmed = await (0, utils_1.confirmLlmUse)(filteredEvaluatorActions, options.force);
        if (!confirmed) {
            return;
        }
        const scores = {};
        const evalRunId = (0, crypto_1.randomUUID)();
        for (const action of filteredEvaluatorActions) {
            const name = (0, utils_1.evaluatorName)(action);
            utils_1.logger.info(`Running evaluator '${name}'...`);
            const response = await runner.runAction({
                key: name,
                input: {
                    dataset: evalDataset,
                    evalRunId,
                },
            });
            scores[name] = response.result;
        }
        const scoredResults = (0, eval_1.enrichResultsWithScoring)(scores, evalDataset);
        const metadata = (0, eval_1.extractMetricsMetadata)(filteredEvaluatorActions);
        const evalRun = {
            key: {
                evalRunId,
                createdAt: new Date().toISOString(),
            },
            results: scoredResults,
            metricsMetadata: metadata,
        };
        utils_1.logger.info(`Writing results to EvalStore...`);
        await evalStore.save(evalRun);
        if (options.output) {
            await exportFn(evalRun, options.output);
        }
    });
});
//# sourceMappingURL=eval-run.js.map